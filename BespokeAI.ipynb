{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPeDt6hcpPa/lm2NEU2IAOb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shere-khan/machine_learning/blob/master/BespokeAI.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Modeling Approach\n",
        "#### Data: Features and formatting\n",
        "The problem could be modeled as such. For a given customer, let $x \\in \\mathbb{R}^p$, be a customer feature vector with $p$ the number of dimensions, and let $y \\in \\mathbb{R}$ be the predicted LTV for a given customer. We call $m$ the time window which considers only the first  $m$ days of data from a customer sign-up, and we use $x$ to predict the LTV $y$ for a given customer. As the first modeling approach, we only consider the data in aggregate. Meaning we construct customer features across the span of $m$ days. Examples of customer features include but are not limited to the following. We will create a 90/10 split for our train/test.\n",
        "\n",
        "#### Things to consider\n",
        "Given a users LTV can change over time, it might be better to model the prediction as a tiered category $y \\in \\{Low, Middle, High\\}$ as an indication for spending potential. Perhaps this is a more natural modeling approach that seeks to categorize a customers spending potential in relation to the spending potential of other customers as opposed to simply outputting LTV as an unbounded range. In the former case, the tiers act as a data structure encoding some kind of understanding of hierarchy that's agnostic to actual value spent. It only cares about value spent in relation to what others have spent. Seems like a silly distinction but might actually have importance. If we were to take this approach, we would use logistic regression in place of linear regression, and our neural network last layer activation would be a softmax. The loss would change to cross entropy. We would have to change the metrics from $R^2$, to AUC, recall, precision, F1 score."
      ],
      "metadata": {
        "id": "egJLGjCn1aRG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Average dollar amount spent per transaction in time window.\n",
        "<br>type: real number\n",
        "\n",
        "- Average number of messages before transaction\n",
        "<br>type: real number\n",
        "- Total amount spent in time window\n",
        "<br>type: real number\n",
        "- Time to first transaction\n",
        "<br>type: real number (days)\n",
        "- Number of transactions\n",
        "<br>type: real number\n",
        "- Most common transaction time (morning, noon, evening, night, early morning) should probably include some notion of time differential\n",
        "<br>type: categorical where the number of categories corresponds to the number of divisions for a 24 hour period.\n",
        "- Average time interval length between transactions\n",
        "<br>type: real number\n",
        "- Repeat customer (number of signups/cancellations to same creator)?\n",
        "<br>type: real number\n",
        "- Number of declined payments?\n",
        "<br>type: real number\n",
        "- Type of transaction \n",
        "<br>type: real number\n",
        "- Num views before transaction\n",
        "<br>type: real number\n",
        "- Total number of messages \n",
        "<br>type: real number\n",
        "- Time subscription was made\n",
        "<br>type: real number\n",
        "- Number of transactions made during live video?\n",
        "<br>type: real number\n",
        "- Message content. Does message content provide us with a signal for identifying potential whales? You mentioned not using message text, but for the sake of completion, it might be worth it to think of something along these lines. For each message (document), use W2V or Glove to get embeddings for words. Average the emebddings for that doc. Then for each vector embedding across documents and customers, you could run k-means with like 10 centroids or something to create clusters. For the actual feature, you could pass those cluster IDs in as a categorical variable. Or you could just average all vectors across documents for a single customer, and concatenate the result to your feature vector for the customer. My theory is the extended generalization achieved through averaging and clustering might washout any potential signal, but still worth it to experiment along those lines.\n",
        "<br>type: $\\mathbb{R}^l$, where $l$ is the number of dimensions of a W2V feature vector, or the number of centroids you choose.\n",
        "<br>\n",
        "- number of tips\n",
        "- number of mass messages\n",
        "- number of customer messages"
      ],
      "metadata": {
        "id": "ji84i8647PAh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Select SQL examples\n",
        "Below are some examples of queries I would write to create the features. I had no data to test with so I'm sure there are some syntax mistakes, but I hope I've communicated the general idea\n",
        "\n"
      ],
      "metadata": {
        "id": "1qxXTPZk5wXw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Messages:\n",
        "# - customerId\n",
        "# - creatorId (assuming this is the creator of the onlyFans hosting content)\n",
        "# - createdAt \n",
        "# - sender (creator or customer) \n",
        "# - messageText\n",
        "\n",
        "# Subscriptions:\n",
        "# - customerId\n",
        "# - creatorId\n",
        "# - subscribedAt\n",
        "# - isExpired (Boolean)\n",
        "\n",
        "# Transactions:\n",
        "# - customerId\n",
        "# - creatorId \n",
        "# - createdAt\n",
        "# - amount\n",
        "# - transactionType (types: tip, mass message, custom message) \n",
        "\n",
        "# total num messages sent to creator\n",
        "select\n",
        "    count(*) as num_message\n",
        "from subscriptions join messages\n",
        "    on subscriptions.customerId = messages.sender\n",
        "where subscribedAt <= date_add(subscribedAt, interval 2 week)\n",
        "group by sender, creatorId\n",
        "\n",
        "\n",
        "# average amount of time bw transactions. The first window to get the time difference\n",
        "# between transactions, and the next window to average over the time difference column.\n",
        "with time_diff_table as (\n",
        "    select\n",
        "        *,\n",
        "        subscribedAt - lag(subscribedAt) over (\n",
        "            partition by subscriptions.creatorId, subscriptions.customerId\n",
        "            order by createdAt) as time_between_transactions\n",
        "    from subscriptions join transactions\n",
        "        on subscriptions.creatorId = transactions.creatorId\n",
        "    where subscribedAt <= date_add(subscribedAt, interval 2 week)\n",
        ")\n",
        "select\n",
        "    avg(time_between_transactions) as avg_time_bw_transactions\n",
        "from time_diff_table\n",
        "group by subscriptions.creatorId, subscriptions.customerId"
      ],
      "metadata": {
        "id": "7Z3Pzbda557p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model selection pros vs cons\n",
        "#### Linear regression\n",
        "We can model the problem using linear regression as such $\\hat{y} = w^\\intercal x$\n",
        "<br>Pros\n",
        "- Interpretable: linear regression is still used because it's simple and interpretable. \n",
        "- Simple: good for baselining\n",
        "<br>\n",
        "\n",
        "Cons\n",
        "- Nonlinearity must be added manually. This goes for interactions between features and standalone features that are nonlinear\n",
        "\n",
        "Things to watch out for:\n",
        "- Make sure your assumptions are correct:\n",
        "<br>\n",
        "i) linearity assumption: plot the residuals and check to see if there is a curve. If so, transform your data using log or exponent.\n",
        "<br>\n",
        "ii) heteroskadasticity: make sure your residuals are normally distributed with constant variance\n",
        "<br>\n",
        "iii) collinearity: Remove correlated features otherwise the standard errors in your parameters could be inflated causing the model to be uninterpretable.\n",
        "<br>\n",
        "iv) check for outliers and high leverage points. This could noise to your data. Noise from high leverage points can be mitigated by either removing the outlier, or scaling a dimension by removing the median and dividing by the 3rd and 1st quantiles.\n",
        "v) Residuals aren't correlated: you can check this by plotting the residuals. The data should not resemble time series data\n",
        "- Features must be scaled or magnitude of corresponding weights might be misleading. That is dimensions of high magnitude relative to other dimensions can lead to weights of high magnitude, signaling potentially undue importance of a feature.\n",
        "\n",
        "#### Gradient Boosting\n",
        "This an ensemble method that essentially creates successive models to correct for residuals, then combines each model additively to yield a prediction.\n",
        "\n",
        "Pros\n",
        "- More powerful than standard linear regression\n",
        "- No need for feature scaling because it uses decision trees\n",
        "- Ensemble methods are more robust to noise aka reduces chances of overfitting\n",
        "- Explainability: Decision tree based methods can yeild feature importance ranking\n",
        "- Learns Nonlinear relationships: By partitioning feature space into smaller and smaller regions\n",
        "Cons\n",
        "- Longer training time perhaps. Maybe not actually because most XGBoost libraries are highly optimized. That could easily be confirmed.\n",
        "\n",
        "#### Neural network\n",
        "Pros\n",
        "- Can learn powerful nonlinear relationships (nth order interactions).\n",
        "\n",
        "Cons\n",
        "- Potentially high training time (though shouldn't have high training time for this data. Simple MLP should get the job done)\n",
        "- More complexity could lead to overfitting. Could use dropout to mitigate this possibility.\n",
        "- Non interpretable\n",
        "- More things to consider when you have a complex model like NNs\n",
        "\n",
        "Things to consider (just a few. This is a simpler model so I won't list all potential things to consider here. Just things I would think to look for)\n",
        "- Still need to scale data. Batch normalization layers solves this problem.\n",
        "- Depth/width (one/two hidden layers should be more than enough as I don't expect there to be much higher order interactions between the features than that. Perhaps W2V embeddings would necessitate more layers in order to catpure interactions but I doubt it.)\n",
        "- what kind of activation functions to use. In this case, any of them should work fine, but need to watch out for vanishing/exploding gradients with certain datasets and architectures. Choosing ReLU is the safest bet. Last layer is a linear activation function of course.\n",
        "- Need to closely observe training loss and take steps accordingly. Are we getting traction on the data? If not, what do we do? Consider learning rate, optimization method (batch, stochastic, RMS prop, adam). Did we overfit? If so, are we using dropout? How much data do we have?"
      ],
      "metadata": {
        "id": "D363QvgZ7Fdr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Offline Training\n",
        "Would use a 90/10 train/test split for all. Validation set can be used when tuning hyperparameters. I would use sklearn or R for training logistic/linear regression as it has a lot of stuff baked in. XGBoost for gradient boosting. I beleive it takes care of the cross validation under the hood. It's a highly optimized library. I would use PyTorch for the MLP, as that's the library I'm most comfortable with. This would require creating a training loop where we run the preset number of epochs, then manually calculate and plot the train/test error and check for overfitting and model stability.\n",
        "\n",
        "## Online Training\n",
        "Say we chose to implement this model in production in order to inform creators about potential spenders. Because the LTV of a customer can change over time, perhaps training on a daily basis on the entire set of customers would be best. We could update the parameters or train a fresh model. Or we can create a holdout set in order to validate the metrics of the model by ensuring there isn't any overfitting etc (everything looks like we expect it to based on offline metrics) before publishing it in prod."
      ],
      "metadata": {
        "id": "KHrrQDgWHB2R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Future work\n",
        "I think the above is a fair baseline, but of course, there is much more we can do with this, although my hyposthesis is that we should get traction with the above first. After getting traction and decent results with the above baseline, a next possible step is to use sequential models. Instead of using the data in aggregate, we would consider time series data and use a model that can take in multiple time steps for a single data point like an autoregressive model or an RNN. That is, one data point would be represented by $m$ days and a $p$ dimensional data point for each day, and instead of constructing features by aggregating over $m$ days, we would consider each day individually."
      ],
      "metadata": {
        "id": "dB35wpmaPWpS"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vk1Scll4PXyX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}